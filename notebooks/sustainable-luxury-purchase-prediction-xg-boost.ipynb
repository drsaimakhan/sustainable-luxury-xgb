{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13638332,"sourceType":"datasetVersion","datasetId":8669089}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ðŸŒ¿ Business & Research Overview\n\nThe luxury market is evolving. Conscious consumers increasingly expect brands to uphold sustainability, transparency, and ethical responsibility â€” transforming sustainability from a â€œnice-to-haveâ€ into a decisive driver of brand equity and purchase intent.\nThis notebook explores how consumer psychology and data science intersect to explain and predict sustainable luxury purchase behavior. Using survey data from 500 respondents, we combine behavioral insights with predictive modeling to uncover what truly influences purchase intention in the sustainable luxury domain.\n\nðŸŽ¯ Project Objective\n\nThis analysis aims to:\n1.\tModel consumer purchase intention toward sustainable luxury products.\n2.\tIdentify the most influential psychological and attitudinal factors driving eco-conscious behavior.\n3.\tExplain model outputs through interpretable AI techniques (SHAP) to reveal why consumers choose sustainably.\n  \nðŸ’¼ Business Relevance\n\nFor brand strategists, marketing analytics teams, and business leaders, this notebook provides a replicable framework for:\nâ€¢\tQuantifying how much sustainability perception impacts buying decisions.\nâ€¢\tDetecting high-value consumer segments based on eco-attitudes and motivation scores.\nâ€¢\tDesigning data-informed marketing campaigns that align sustainability with brand desirability.\nThe findings can directly inform product positioning, communication strategy, and CSR alignment for luxury houses seeking growth through responsibility.\n\nðŸ§  Technical Approach\n\nWe evaluate several machine learning models â€” including Linear Regression, Random Forest, XGBoost, and Optuna-tuned ensembles â€” to predict consumer responses.\nModel transparency is ensured through:\nâ€¢\tFeature importance visualizations (Random Forest & XGBoost)\nâ€¢\tSHAP explainability to trace feature contributions at both global and individual levels\nAll models are wrapped in scikit-learn pipelines with robust preprocessing, ensuring clean, reproducible experimentation.\n\nðŸ“ˆ Key Insights\n\nâ€¢\tAttitudinal items (ATT1â€“ATT5) are dominant predictors of sustainability preference.\nâ€¢\tPersonality traits contribute marginally but help segment nuanced consumer profiles.\nâ€¢\tSHAP visualizations translate complex models into actionable business understanding â€” showing why certain traits or attitudes lead to purchase intent.\n\nðŸŒ Why This Matters\n\nAs sustainability transitions from trend to expectation, data-driven empathy becomes the new competitive edge.\nThis notebook demonstrates how marketing science + machine learning can quantify the emotional and cognitive dimensions of sustainability â€” empowering decision-makers to build brands that are not only luxurious, but also responsible and future-ready.\n","metadata":{}},{"cell_type":"code","source":"# ============================\n# Sustainable Luxury: Full ML Pipeline\n# ============================\n\n# ---- Imports\nimport os, math, sys, warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import (\n    mean_absolute_error, mean_squared_error, r2_score,\n    classification_report, accuracy_score, f1_score\n)\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.exceptions import UndefinedMetricWarning\nfrom scipy.sparse import issparse\nimport joblib\n\n# Notebook niceties\npd.set_option(\"display.max_columns\", 120)\n\n# If in a notebook, enable inline plotting (safe-guarded)\ntry:\n    get_ipython\n    # This IPython magic is fine in Kaggle notebooks\n    # If you convert to .py, remove the next line.\n    get_ipython().run_line_magic(\"matplotlib\", \"inline\")\nexcept Exception:\n    pass\n\n# ---- Silence specific sklearn warning\nwarnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n\n# ============================\n# 1) Load Excel robustly\n# ============================\ncandidate_paths = [\n    \"/kaggle/working/Sustainable_Luxury_Survey_500.xlsx\",\n    \"/kaggle/working/sustainable_luxury_survey.xlsx\",\n    \"/kaggle/working/Sustainable_Luxury_Survey.xlsx\",\n]\n\n# Also scan /kaggle/input for Excel files if you added a Kaggle Dataset\ninput_dir = \"/kaggle/input\"\nif os.path.exists(input_dir):\n    for root, _, files in os.walk(input_dir):\n        for f in files:\n            if f.lower().endswith((\".xlsx\", \".xls\")):\n                candidate_paths.append(os.path.join(root, f))\n\nprint(\"Candidate paths:\", candidate_paths)\n\ndf = None\nfor p in candidate_paths:\n    if os.path.exists(p):\n        try:\n            df = pd.read_excel(p)\n            print(f\"Loaded: {p} shape={df.shape}\")\n            break\n        except Exception as e:\n            print(\"Tried\", p, \"->\", e)\n\nif df is None:\n    raise FileNotFoundError(\n        \"No Excel found. Upload your file via the left sidebar (ðŸ“ Upload) and re-run.\"\n    )\n\n# ============================\n# 2) Normalize column names\n# ============================\ndf = df.rename(columns=lambda c: str(c).strip().replace(\" \", \"_\").replace(\"-\", \"_\").lower())\nprint(\"Columns:\", df.columns.tolist())\n\n# ============================\n# 3) Pick targets by common aliases\n# ============================\ndef pick_first_existing(columns, candidates):\n    for c in candidates:\n        if c in columns:\n            return c\n    return None\n\nsust_candidates = [\n    \"sustainability_attitude\", \"sustainability_attitude_score\",\n    \"sustainability_preference\", \"sustainability_preference_score\",\n    \"attitude_toward_sustainability\", \"attitude_score\"\n]\npurchase_candidates = [\n    \"purchase_intent\", \"purchase_intention\", \"purchase_intention_score\",\n    \"purchase_intent_score\"\n]\n\nsust_target = pick_first_existing(df.columns, sust_candidates)\npurchase_target = pick_first_existing(df.columns, purchase_candidates)\n\nif not sust_target or not purchase_target:\n    raise KeyError(\n        f\"Could not find sustainability and/or purchase targets.\\n\"\n        f\"Columns present: {df.columns.tolist()}\\n\"\n        f\"Expected one of {sust_candidates} and one of {purchase_candidates}.\"\n    )\n\nprint(f\"Targets -> sustainability: {sust_target} | purchase: {purchase_target}\")\n\n# ============================\n# 4) Build features / targets\n# ============================\nid_like = [c for c in df.columns if c in [\"id\", \"respondent_id\"]]\nX = df.drop(columns=[sust_target, purchase_target] + id_like, errors=\"ignore\")\n\ny_map = {\n    \"sustainability\": df[sust_target],\n    \"purchase\": df[purchase_target],\n}\n\nnum_cols = X.select_dtypes(include=[\"number\"]).columns.tolist()\ncat_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\nprint(\"Numeric features:\", num_cols[:10], f\"... ({len(num_cols)} total)\")\nprint(\"Categorical features:\", cat_cols)\n\n# ============================\n# 5) Preprocessing + models\n# ============================\nnumeric_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    # with_mean=False keeps it sparse-safe when combined with OneHot\n    (\"scaler\", StandardScaler(with_mean=False)),\n])\n\ncategorical_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n])\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, num_cols),\n        (\"cat\", categorical_transformer, cat_cols),\n    ],\n    remainder=\"drop\",\n)\n\nreg_model = RandomForestRegressor(\n    n_estimators=400, random_state=42, n_jobs=-1\n)\n\nclf_model = RandomForestClassifier(\n    n_estimators=400, random_state=42, n_jobs=-1\n)\n\n# ============================\n# 6) Regression for both targets\n# ============================\ndef rmse(y_true, y_pred):\n    return math.sqrt(mean_squared_error(y_true, y_pred))\n\nprint(\"\\n=== REGRESSION RESULTS ===\")\nresults_reg = {}\n\nfor name, y in y_map.items():\n    if not np.issubdtype(y.dtype, np.number):\n        print(f\"[{name}] target is not numeric. Skipping regression for this target.\")\n        continue\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    reg_pipe = Pipeline(steps=[(\"prep\", preprocess), (\"model\", reg_model)])\n    reg_pipe.fit(X_train, y_train)\n\n    preds = reg_pipe.predict(X_test)\n    mae = mean_absolute_error(y_test, preds)\n    r2 = r2_score(y_test, preds)\n    score_rmse = rmse(y_test, preds)\n\n    print(f\"[{name}] MAE={mae:.3f} | RMSE={score_rmse:.3f} | R^2={r2:.3f}\")\n\n    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n    cv_scores = cross_val_score(reg_pipe, X, y, cv=cv, scoring=\"r2\", n_jobs=-1)\n    print(f\"[{name}] 5-fold CV R^2: {np.round(cv_scores,3)} | mean={cv_scores.mean():.3f}\")\n\n    results_reg[name] = dict(MAE=mae, RMSE=score_rmse, R2=r2, CV_R2_mean=cv_scores.mean())\n\n# ============================\n# 7) Optional classification: bin targets into Low/Med/High\n# ============================\ndef bin_target(series, labels=[\"Low\", \"Med\", \"High\"]):\n    \"\"\"\n    Use quantile bins so classes are more balanced and\n    less likely to be empty -> avoids undefined metrics.\n    \"\"\"\n    return pd.qcut(series, q=3, labels=labels, duplicates=\"drop\")\n\nprint(\"\\n=== CLASSIFICATION RESULTS (Low/Med/High) ===\")\nresults_clf = {}\n\nfor name, y_raw in y_map.items():\n    if not np.issubdtype(y_raw.dtype, np.number):\n        print(f\"[{name}] target is not numeric. Skipping classification.\")\n        continue\n\n    # Make categorical with likely non-empty bins\n    y_cat = bin_target(y_raw).astype(\"category\")\n    y_cat = y_cat.cat.remove_unused_categories()\n\n    # Train/test split (stratify preferred; fallback handled)\n    try:\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y_cat, test_size=0.2, random_state=42, stratify=y_cat\n        )\n    except ValueError:\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y_cat, test_size=0.2, random_state=42\n        )\n\n    clf_pipe = Pipeline(steps=[(\"prep\", preprocess), (\"model\", clf_model)])\n    clf_pipe.fit(X_train, y_train)\n    preds = clf_pipe.predict(X_test)\n\n    # Ensure consistent label order for reports\n    labels_order = list(y_test.cat.categories) if hasattr(y_test, \"cat\") else sorted(pd.unique(y_test))\n\n    acc = accuracy_score(y_test, preds)\n    f1w = f1_score(y_test, preds, average=\"weighted\", zero_division=0)\n    print(f\"[{name}] Acc={acc:.3f} | F1_w={f1w:.3f}\")\n\n    print(\n        f\"\\nClassification report ({name}):\\n\",\n        classification_report(\n            y_test,\n            preds,\n            labels=labels_order,\n            zero_division=0   # <- prevents UndefinedMetricWarning\n        )\n    )\n\n    results_clf[name] = dict(Accuracy=acc, F1_weighted=f1w)\n\n# ============================\n# 8) Feature importance (approx)\n# ============================\nprint(\"\\n=== FEATURE IMPORTANCES (approximate) ===\")\n\n# Fit preprocess on ALL X to extract stable feature names\npreprocess_fitted = preprocess.fit(X)\n\nfeature_names_num = num_cols\nfeature_names_cat = []\nif len(cat_cols) > 0:\n    feature_names_cat = preprocess_fitted.named_transformers_[\"cat\"][\"onehot\"] \\\n        .get_feature_names_out(cat_cols).tolist()\nfeature_names = feature_names_num + feature_names_cat\n\nX_encoded = preprocess_fitted.transform(X)\nX_encoded_dense = X_encoded.toarray() if issparse(X_encoded) else X_encoded\n\nfor name, y in y_map.items():\n    if not np.issubdtype(y.dtype, np.number):\n        print(f\"[{name}] target non-numeric -> skipping feature importance.\")\n        continue\n\n    rf = RandomForestRegressor(n_estimators=400, random_state=42, n_jobs=-1)\n    rf.fit(X_encoded_dense, y)\n\n    imps = pd.Series(rf.feature_importances_, index=feature_names).sort_values(ascending=False)\n    topk = imps.head(15)\n\n    print(f\"\\nTop features for [{name}]:\")\n    display(topk.to_frame(\"importance\"))\n\n    plt.figure()\n    topk.sort_values().plot(kind=\"barh\")\n    plt.title(f\"Top Feature Importances for {name}\")\n    plt.xlabel(\"Importance\")\n    plt.gca().invert_yaxis()\n    plt.show()\n\n# ============================\n# 9) Save trained full-data regressors\n# ============================\nout_dir = \"/kaggle/working/models\"\nos.makedirs(out_dir, exist_ok=True)\n\nfor name, y in y_map.items():\n    if not np.issubdtype(y.dtype, np.number):\n        continue\n    pipe_full = Pipeline(steps=[\n        (\"prep\", preprocess),\n        (\"model\", RandomForestRegressor(n_estimators=400, random_state=42, n_jobs=-1))\n    ])\n    pipe_full.fit(X, y)\n    joblib.dump(pipe_full, os.path.join(out_dir, f\"rf_regressor_{name}.joblib\"))\n\nprint(\"Saved models to:\", out_dir)\n\n# ============================\n# 10) Quick EDA prints\n# ============================\nprint(\"\\n=== BASIC EDA ===\")\nprint(\"Shape:\", df.shape)\n\nprint(\"\\nNumeric summary:\")\ndisplay(df.describe(include=\"number\"))\n\nprint(\"\\nCategorical summary:\")\ndisplay(df.describe(include=\"object\"))\n\n# ============================\n# 11) SHAP EXPLANATIONS (Sustainability model)\n# ============================\nimport shap\n\n# Refit a pipeline on full data for \"sustainability\" regression (numeric check)\nif np.issubdtype(y_map[\"sustainability\"].dtype, np.number):\n    shap_pipe = Pipeline(steps=[(\"prep\", preprocess), (\"model\", reg_model)])\n    shap_pipe.fit(X, y_map[\"sustainability\"])\n\n    # Encoded features\n    X_enc = shap_pipe.named_steps[\"prep\"].transform(X)\n    X_enc_dense = X_enc.toarray() if issparse(X_enc) else X_enc\n\n    # TreeExplainer for RandomForestRegressor\n    explainer = shap.TreeExplainer(shap_pipe.named_steps[\"model\"])\n    shap_values = explainer.shap_values(X_enc_dense)\n\n    # Build feature name list again (safe for no-cats)\n    feature_names_shap = num_cols[:]\n    if len(cat_cols):\n        feature_names_shap += preprocess_fitted.named_transformers_[\"cat\"][\"onehot\"] \\\n            .get_feature_names_out(cat_cols).tolist()\n\n    # Summary plot\n    shap.summary_plot(shap_values, features=X_enc_dense, feature_names=feature_names_shap)\n\n    # Dependence plot (pick a sensible feature that exists)\n    # If \"att1\" doesn't exist, fall back to the first numeric column\n    dep_feat = \"att1\" if \"att1\" in feature_names_shap else (feature_names_shap[0] if len(feature_names_shap) else None)\n    if dep_feat is not None:\n        shap.dependence_plot(dep_feat, shap_values, X_enc_dense, feature_names=feature_names_shap)\n\n    # Waterfall / force for one respondent\n    sample_idx = 0\n    shap.initjs()\n\n    # New SHAP Explanation object (waterfall)\n    try:\n        exp = shap.Explanation(\n            values=shap_values[sample_idx],\n            base_values=explainer.expected_value,\n            data=X_enc_dense[sample_idx],\n            feature_names=feature_names_shap\n        )\n        shap.plots.waterfall(exp, max_display=15)\n    except Exception as e:\n        print(\"Waterfall explanation failed:\", e)\n\n    # Matplotlib force plot fallback\n    try:\n        shap.force_plot(\n            base_value=explainer.expected_value,\n            shap_values=shap_values[sample_idx, :],\n            features=X_enc_dense[sample_idx, :],\n            feature_names=feature_names_shap,\n            matplotlib=True\n        )\n    except Exception as e:\n        print(\"Force plot failed:\", e)\nelse:\n    print(\"Sustainability target is non-numeric; skipping SHAP.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:49:57.593782Z","iopub.execute_input":"2025-11-06T16:49:57.595158Z","iopub.status.idle":"2025-11-06T16:50:27.657917Z","shell.execute_reply.started":"2025-11-06T16:49:57.595122Z","shell.execute_reply":"2025-11-06T16:50:27.657165Z"}},"outputs":[],"execution_count":null}]}